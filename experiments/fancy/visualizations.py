import subprocess
from termcolor import colored

from fancy.frequencies_and_opportunities import *
from fancy.system_performance_lib import *
from fancy.info_extractors import *
from fancy.plot import *
import copy

COLORS = ["#0C8C20", "#348ABD", "#A60628", "#7A68A6",
          "#467821", "#CF4457", "#188487", "#E24A33"]
COLORS = ['#1f77b4',
          '#ff7f0e',
          '#2ca02c',
          '#d62728',
          '#9467bd',
          '#8c564b',
          '#e377c2',
          '#7f7f7f',
          '#bcbd22',
          '#17becf']


paper_colors = ['#2f79a7',  # 0 blue
                '#A4251C',  # 1 red
                '#d5ed49',  # 4 yellow green
                'orange',  # 5 orange
                '#666666',  # 6 dark grey
                '#573c06',  # 7 brown
                '#c9c212',  # 2 yellow
                '#003300',  # 3 dark green
                ]

colors2 = ['#e41a1c',
           '#377eb8',
           '#4daf4a',
           '#984ea3',
           '#ff7f00',
           '#ffff33',
           '#a65628',
           '#f781bf']

colors3 = [
    '#e6194b',
    '#3cb44b',
    '#ffe119',
    '#0082c8',
    '#f58231',
    '#911eb4',
    '#46f0f0',
    '#f032e6',
    '#d2f53c',
    '#fabebe',
    '#008080',
    '#e6beff'
]


def check_prefix_by_prefix(
        inputs_dir_base, outputs_dir_base, slice=0, num_top_prefixes=500,
        send_duration=5, top_speed=20, zooming_speed=50, tree_depth=5,
        plot=False, info=None):
    """
    Args:
        inputs_dir_base: path to input files to the simulation
        outputs_dir_base:  path to output files generated by the simulation
        num_top_prefixes: number of top prefixes in the system (?)
        sim_duration: duration of the simulation
        top_speed: speed of top prefixes
        zooming_speed: speed of zooming
        tree_depth: three depths

    Returns: (so far prints something)
    NOTE: even if we exchange every Xms, depending on when the machine started, prefixes can be detected between 0 and Xms
    """

    slice_top_prefixes = load_top_prefixes_dict(
        inputs_dir_base + "_{}.top".format(slice))
    global_top_prefixes = load_top_prefixes_dict(inputs_dir_base + ".top")

    failed_prefixes = load_failed_prefixes(
        outputs_dir_base + "-failed_prefixes.txt")
    sim_out = load_simulation_out(outputs_dir_base + "_s1.json")

    trace_ts = load_trace_ts(inputs_dir_base + "_{}.info".format(slice))
    prefixes_ts = load_prefixes_ts_raw(
        inputs_dir_base + "_{}.ts".format(slice))

    start_ts = trace_ts[0]
    end_ts = start_ts + Decimal(send_duration)

    # load packet frequencies caches to be faster
    # caching is not needed anymore
    # load_prefixes_packet_frequency(inputs_dir_base + "_{}".format(slice), send_duration, list(slice_top_prefixes.keys())[:2500], [5,10])
    cached_prefixes_packet_frequency = {}

    total_packets = 0
    total_packets_detected = 0

    total_packets_sim = 0
    total_packets_sim_detected = 0

    total_prefixes_detected = 0

    # number of zooming opprotunites per prefix and zooming speed this is saved as we process prefixes
    zooming_opportunities_data = []

    header_str = colored(
        "\nSimulation Information:\nSystem Top Prefixes={}, Send Duration={}, Top Prefixes Speed={}, Zooming Speed={},"
        " Tree Depth={}\nSyemTopPrefixes={}, TopEntriesTrace={}, TopPrefixesFailed={}, Type={}, BottomPrefixesFailed={}, Type={}, CostType={}\n".
        format(
            num_top_prefixes, send_duration, top_speed,
            zooming_speed, tree_depth,
            info["NumTopEntriesSystem"],
            info["NumTopEntriesTraffic"],
            info["NumTopFails"],
            info["TopFailType"],
            info["NumBottomFails"],
            info["BottomFailType"],
            info["CostType"]),
        "green")
    print(header_str)

    title = "{:>4} {:>7} {:>7} {:>6} {:>15} {:>3} {:>12} {:>8} | {:>8} | {:>4} {:>4} {:>4} | {:>4} {:>4} {:>4} | {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} | {:>6} {:>6} {:>6} {:>6}". format(
        "num", "g_top", "s_top", "id", "prefix", "typ", "tot_bytes", "tot_pkt", "pkts", "wmc", "wzo", "wtb", "smc", "szo", "stb", "fail_t", "disc_t", "f_pkt", "f_wzo", "f_szo", "l_pkt", "bwzoom", "bwzo", "bszoom", "bszo")

    separator = len(title) * "-"
    print(title)
    print(separator)

    prefix_num = 0
    prefixes_detection_times = {}

    for prefix in failed_prefixes:

        prefix_num += 1
        global_top, global_bytes, global_packets = global_top_prefixes[prefix]
        slice_top, slice_bytes, slice_packets = slice_top_prefixes[prefix]

        # get metadata
        id, _type = get_prefix_type(
            prefix, global_top_prefixes, num_top_prefixes)
        # get detection time
        detection_time = get_prefix_detection_time(
            prefix, _type, id, sim_out['failures'])

        # this will not be like this once we add multiple trees
        if _type == 1:
            speed = Decimal(top_speed) / 1000
        else:
            speed = Decimal(zooming_speed) / 1000

        prefix_ts = prefixes_ts.get(prefix, None)
        if prefix_ts:
            if type(prefix_ts) == dict:
                prefix_ts = prefix_ts['ts']

        cached_frequencies = cached_prefixes_packet_frequency.get(prefix, None)
        packet_frequency, num_packets = get_prefix_packet_frequency(
            prefix_ts, start_ts, end_ts, speed, cached_frequencies)

        # gets the performance for a specific frequency and tree set up
        weak_total_bins, \
            weak_max_consecutive_bins, \
            weak_zooming_opportunities, \
            weak_first_zooming_opportunity, \
            strong_total_bins, \
            strong_max_consecutive_bins, \
            strong_zooming_opportunities, \
            strong_first_zooming_opportunity = prefix_zooming_performance(packet_frequency, tree_depth, WEAK_BIN, STRONG_BIN)

        speeds = range(5, 1000, 5)
        best_weak_zooming, best_strong_zooming, zooming_opportunities = get_prefix_best_zooming_speed(
            prefix_ts, start_ts, end_ts, tree_depth, zooming_speeds=speeds, cached_frequencies=cached_frequencies)

        zooming_opportunities_data.append(
            {"detection_time": detection_time, "prefix": prefix, "type": _type,
             "zooming_opportunities": zooming_opportunities})

        failure_time = 0

        # for this specific prefix
        first_ts = prefix_ts[0] - start_ts
        if first_ts > send_duration:
            first_ts = -1

        last_ts = find_last_ts_before(
            prefix_ts, start_ts + Decimal(send_duration))
        if last_ts != -1:
            last_ts -= start_ts
        #prefix_ts[-1] - start_ts

        if weak_first_zooming_opportunity != -1:
            weak_first_zooming_opportunity -= start_ts

        if strong_first_zooming_opportunity != -1:
            strong_first_zooming_opportunity -= start_ts

        info = "{:>4} {:>7} {:>7} {:>6} {:>15} {:>3} {:>12} {:>8} | {:>8} | {:>4} {:>4} {:>4} | {:>4} {:>4} {:>4} | {:>8.5f} {:>8.5f} {:>8.5f} {:>8.5f} {:>8.5f} {:>8.5f} | {:>6} {:>6} {:>6} {:>6}".format(
            prefix_num, global_top, slice_top,
            id, prefix,
            _type, global_bytes, global_packets,
            num_packets,
            weak_max_consecutive_bins,
            weak_zooming_opportunities,
            weak_total_bins,
            strong_max_consecutive_bins,
            strong_zooming_opportunities,
            strong_total_bins,
            failure_time, max(detection_time - 2, -1),
            first_ts, weak_first_zooming_opportunity,
            strong_first_zooming_opportunity, last_ts,
            best_weak_zooming[0], best_weak_zooming[1],
            best_strong_zooming[0], best_strong_zooming[1])

        """
        Collect info for the plots
        """
        prefixes_detection_times[prefix] = {
            "failure_time": failure_time,
            "detection_time": max(detection_time - 2, -1),
            "first_packet": first_ts,
            "first_wzo": weak_first_zooming_opportunity, "bytes": slice_bytes,
            "packets": num_packets}

        total_packets += global_packets
        total_packets_sim += num_packets

        if detection_time != -1:
            total_packets_detected += global_packets
            total_packets_sim_detected += num_packets
            total_prefixes_detected += 1
            str_out = colored(info, "green")

        else:
            str_out = colored(info, "red")

        print(str_out)

    print(
        "\nPrefixes detected stats:\n{}\nPrefixes Detected: {}({}/{})\nGlobal Packet Share: {}({}/{})\nSimulation Packet Share: {}({}/{}) ".
        format(
            len("Prefixes detected stats:") * "-",
            float(total_prefixes_detected) / len(failed_prefixes),
            total_prefixes_detected, len(failed_prefixes),
            float(total_packets_detected) / total_packets,
            total_packets_detected, total_packets,
            float(total_packets_sim_detected) / total_packets_sim,
            total_packets_sim_detected, total_packets_sim))

    """
    Getting info and plotting it
    """
    if plot:
        # total_zooming_opportunities, detected_zooming_opportunities, not_detected_zooming_opportunities = get_zooming_opportunities_per_type_with_cap(zooming_opportunities_data, 1)
        total_zooming_opportunities, detected_zooming_opportunities, not_detected_zooming_opportunities = get_zooming_opportunities_per_type_with_only_top_x(
            zooming_opportunities_data, 25)

        shares = get_detection_shares_info(
            sim_out, failed_prefixes, slice_top_prefixes, global_top_prefixes,
            num_top_prefixes)

        detection_times, prefix_info, * \
            detection_times_cdf, percentile95 = get_detection_times_cdf_info(prefixes_detection_times)

        # getting performance info
        tpr, fpr, tnr, fnr, fp, tp = get_run_performance(
            sim_out, failed_prefixes, len(slice_top_prefixes),
            global_top_prefixes, num_top_prefixes)

        # get plot layout
        fig = plt.figure(figsize=(10, 10))

        zooming_opportunities_ax1 = fig.add_subplot(3, 2, 1)
        zooming_opportunities_ax2 = fig.add_subplot(3, 2, 3)
        zooming_opportunities_ax3 = fig.add_subplot(3, 2, 5)

        detection_time_cdf_ax = fig.add_subplot(3, 2, 2)
        performance_ax = fig.add_subplot(3, 2, 4)
        shares_ax = fig.add_subplot(3, 2, 6)

        # plotting to axes
        axes = [
            zooming_opportunities_ax1, zooming_opportunities_ax2,
            zooming_opportunities_ax3]
        plot_zooming_opportunities(
            axes,
            [total_zooming_opportunities, detected_zooming_opportunities,
             not_detected_zooming_opportunities])
        # , outputs_dir_base+"_image.pdf")

        plot_detection_time_cdf(detection_time_cdf_ax, detection_times_cdf)

        plot_performance_metrics(performance_ax, (tpr, fpr, tnr, fnr))

        plot_performance_shares(shares_ax, shares)

        fig.tight_layout()
        plt.savefig(outputs_dir_base + "_image.pdf")

# move to visualization


def check_prefix_by_prefix_from_info(info_file):
    """
    Helper to call check prefix by prefix from info
    Args:
        info_file:

    Returns:

    """

    # load info file
    import subprocess
    sim_info = load_sim_info_file(info_file)

    hostname = subprocess.check_output("hostname", shell=True)
    hostname = str(hostname, 'utf-8').strip()

    if 'edgar' in hostname:
        sim_info["InDirBase"] = sim_info["InDirBase"].replace(
            "/home/cedgar/", "/Users/edgar/")
        sim_info["OutDirBase"] = sim_info["OutDirBase"].replace(
            "/home/cedgar/ns3-offloading/",
            "/Users/edgar/p4-offloading/simulation/")

    check_prefix_by_prefix(
        sim_info["InDirBase"],
        sim_info["OutDirBase"],
        slice=int(sim_info["TraceSlice"]),
        num_top_prefixes=int(sim_info["NumTopEntriesSystem"]),
        send_duration=float(sim_info["SendDuration"]),
        top_speed=float(sim_info["ProbingTimeTopEntriesMs"]),
        zooming_speed=float(sim_info["ProbingTimeZoomingMs"]),
        tree_depth=int(sim_info["TreeDepth"]),
        info=sim_info)


"""
Detectable Prefixes Info scripts

Following two scripts compute detectable info files, those outputs
are then used by many other scripts.
"""


def get_detectable_prefixes_info(
        trace_base_path, output_base_path, tree_depth, speeds, min_weak,
        min_strong, duration=0, only_detectable=True, loss_rate=1):
    """

    Args:
        trace_base_path:
        output_base_path:
        tree_depth:
        speeds:
        min_weak:
        min_strong:
        duration:
        only_detectable:

    Returns:

    """
    # IMPORTANT TEMPORAL PATCH TO MANUALLY MODIFY TRACE_BASE_PATH
    # This is needed because .top and .ts .info do not have the same base:
    if "per_flow_retrans" in trace_base_path:
        trace_base_path_top = trace_base_path.replace("per_flow_retrans_", "")
    else:
        trace_base_path_top = trace_base_path

    try:
        print("Start Get Detectable Prefixes: {} {} {}".format(
            trace_base_path, tree_depth, loss_rate))
        all_prefixes = list(load_top_prefixes_dict(
            trace_base_path_top + ".top").keys())
        num_all_prefixes = len(all_prefixes)

        raw_opportunities = get_raw_zooming_opportunities(
            trace_base_path, all_prefixes, tree_depth, speeds, duration, loss_rate)

        if only_detectable:
            raw_opportunities = get_only_detectable_raw_opportunities(
                raw_opportunities)

        speed_to_detectable_prefixes = get_detectable_prefixes_per_speed(
            raw_opportunities,
            min_weak,
            min_strong)

        # Save info so we can use later
        # do not save raw we dont needed it, will speed this a bit
        #pickle.dump(raw_opportunities, open(output_base_path + ".raw", "wb"))
        pickle.dump(speed_to_detectable_prefixes, open(
            output_base_path + ".detect", "wb"))

        print("Finish Get Detectable Prefixes: {} {} {}".format(
            trace_base_path, tree_depth, loss_rate))

    except:
        print("FAILED!: Start Get Detectable Prefixes: {} {} {}".format(
            trace_base_path, tree_depth, loss_rate))
        import traceback
        traceback.print_exc()

    # interesting but we wont use right now
    #detectable_intersect = get_detectable_prefixes_intersect(speed_to_detectable_prefixes, speeds_to_ms_decimals(speeds))


# out_path_base=/mnt/fischer/cedgar/fancy_outputs/detectable_prefixes_info/"
def run_many_get_detectable_prefixes(
        processors=24, in_traces=None, out_path_base=""):

    # create output path in case it does not exist
    subprocess.call("mkdir -p {}".format(out_path_base), shell=True)

    if in_traces:
        traces = in_traces
    else:
        traces = ['equinix-chicago.dirA.20160121',
                  'equinix-chicago.dirB.20140619', 'equinix-nyc.dirA.20180419',
                  'equinix-chicago.dirB.20131219',
                  'equinix-chicago.dirB.20160121',
                  'equinix-sanjose.dirA.20131024']

    depths = [2, 3, 4]
    durations = [30]
    #packet_losses = [0.01, 0.05, 0.1, 0.5, 0.75, 1]
    packet_losses = [1]
    #slices = [1,2,3,4,6,7,8]
    slices = range(10)
    slices = range(5, 10)
    #slices = range(5)

    #speeds = list(range(5, 2001, 5))
    speeds = range(5, 1001, 5)
    opportunity_mins = [(1, 1)]

    pool = multiprocessing.Pool(processors)

    for trace in traces:
        for depth in depths:
            for duration in durations:
                for slice in slices:
                    for packet_loss in packet_losses:
                        for opportunity_min in opportunity_mins:
                            input_base = "/mnt/fischer/cedgar/fancy_traces/{}/{}_per_flow_retrans_{}".format(
                                trace, trace, slice)
                            #input_base = "/mnt/fischer/cedgar/fancy_traces/{}/{}_{}".format(trace, trace, slice)
                            output_base = "{}/{}_{}_{}_{}_{}_{}_{}".format(
                                out_path_base,
                                trace,
                                depth,
                                duration,
                                slice,
                                opportunity_min
                                [0],
                                opportunity_min
                                [1],
                                packet_loss)

                            #get_detectable_prefixes_info(input_base, output_base, depth, speeds, opportunity_min[0], opportunity_min[1], duration, True, packet_loss)
                            # return
                            pool.apply_async(
                                get_detectable_prefixes_info,
                                (input_base, output_base, depth, speeds,
                                 opportunity_min[0],
                                 opportunity_min[1],
                                 duration, True, packet_loss),
                                {})

    pool.close()
    pool.join()


"""
"""


def display_detectable_info(
        detectable_info_file, top_prefixes_file, output_file,
        top_prefixes_to_check, speeds=None,
        loss_rates=[0.01, 0.05, 0.1, 0.5, 0.75, 1],
        max_trees=2, max_combinations=100000):
    """
    Displays info about detectable prefixes. Uses Detectable prefixes raw that we saved somewhere.
    Args:
        detectable_info_file:
        top_prefixes_file:
        output_file:
        top_prefixes_to_check:
        max_trees:
        max_combinations:

    Returns:

    """

    top_prefixes = load_top_prefixes_dict(top_prefixes_file)
    num_total_prefixes = len(top_prefixes)
    num_total_bytes, num_total_packets = get_prefixes_accumulated_share(
        top_prefixes, top_prefixes.keys())
    out_file = open(output_file, "w")

    for loss_rate in loss_rates[::-1]:

        detectable_file_name = detectable_info_file + "_{}.detect".format(
            loss_rate)
        speed_to_detectable_prefixes = load_speed_to_detectable_from_caches(
            detectable_file_name,
            speeds)
        detectable_prefixes, _ = get_all_prefixes_from_detectable(
            speed_to_detectable_prefixes)

        # not used since we do it at every iteration of the for loop
        num_detectable_prefixes = len(detectable_prefixes)

        steps = top_prefixes_to_check.copy()

        steps.append(num_total_prefixes)
        steps = sorted(steps)
        steps = steps[:steps.index(num_total_prefixes) + 1]

        title = ("Trace name: {}".format(detectable_file_name.split("/")[-1]))
        print(title)
        out_file.write(title + "\n")
        print("-" * len(title))
        out_file.write("-" * len(title) + "\n")

        title_tree = ["Detect Tree {}".format(x + 1) for x in range(max_trees)]
        title = ("{:>20} | {:>20} | {:>28} " + "| {:>38} " * max_trees + "|").format(
            "# top prefix", "All Prefixes", "Detectable(all)", *title_tree)
        print(title)
        out_file.write(title + "\n")
        print("-" * len(title))
        out_file.write("-" * len(title) + "\n")
        #"Detect X tree(avg)", "Detect X tree(best%)",

        for step in steps:
            s = "{:>20} | {:>20} | {:>28} " + "| {:>38} " * max_trees + "|"

            prefixes = get_list_top_prefixes(top_prefixes_file, step)

            # Total prefixes Info
            prefixes_share = step / num_total_prefixes
            bytes, packets = get_prefixes_accumulated_share(
                top_prefixes, prefixes)
            bytes_share = bytes / num_total_bytes
            packets_share = packets / num_total_packets

            total_prefixes_info = "{:.4f}/{:.4f}/{:.4f}".format(
                prefixes_share, bytes_share, packets_share)

            # Total Detectable Info
            detectable_prefixes_step = set(
                prefixes).intersection(set(detectable_prefixes))

            prefixes_share = len(detectable_prefixes_step) / num_total_prefixes
            bytes, packets = get_prefixes_accumulated_share(
                top_prefixes, detectable_prefixes_step)
            bytes_share = bytes / num_total_bytes
            packets_share = packets / num_total_packets

            detectable_prefixes_info = "{:.4f}({})/{:.4f}/{:.4f}".format(
                prefixes_share, len(detectable_prefixes_step), bytes_share, packets_share)

            # Tree stuff..
            subset_detectable_prefixes_raw = get_detectable_prefixes_subset(
                speed_to_detectable_prefixes, prefixes)

            detectable_prefixes_info_tree = []
            for trees in range(max_trees):
                # gets the best in performance, but not the best in zooming speed,
                # we should allow some tollerance of 0.1% of something suck we can lower it
                best = get_best_tree_combination(
                    subset_detectable_prefixes_raw,
                    trees + 1, max_combinations)

                prefixes_share = best[0] / len(detectable_prefixes_step)
                bytes, packets = get_prefixes_accumulated_share(
                    top_prefixes, best[1])
                bytes_share = bytes / num_total_bytes
                packets_share = packets / num_total_packets

                mean_speeds = np.mean(best[2])

                detectable_prefixes_info_tree.append(
                    "{:.4f}({}/{:.4f})/{:.4f}/{:.4f}".format(
                        prefixes_share, best[0],
                        mean_speeds, bytes_share, packets_share))

            s = s.format(
                step, total_prefixes_info, detectable_prefixes_info, *
                detectable_prefixes_info_tree)
            print(s)
            out_file.write(s + "\n")

        print()
        out_file.write("\n")


def many_display_detectable_info(
        detectable_prefixes_base_path, top_prefixes_base_path,
    output_base_path, slices, top_prefixes_to_check, loss_rates,
        depths=[3, 4],
        max_trees=2, max_combinations=5000, processors=20):

    traces = all_traces

    os.system("mkdir -p {}".format(output_base_path))
    pool = multiprocessing.Pool(processors)

    speeds = [range(5, 1001, 5), range(5, 2001, 5)]

    for trace in traces:
        for depth in depths:
            for slice in slices:
                for speed in speeds:
                    max_zoom = round(speed[-1] / 1000)

                    detectable_prefixes_input = "{}/{}_{}_{}_{}_{}_{}".format(
                        detectable_prefixes_base_path, trace, depth, 30, slice, 1, 1)
                    top_prefixes_input = "{}/{}/{}_{}.top".format(
                        top_prefixes_base_path, trace, trace, slice)
                    output_name = "{}/{}_{}_{}_{}_{}_{}_{}.performance".format(
                        output_base_path, trace, depth, 30, max_zoom, slice, 1, 1)

                    print(detectable_prefixes_input,
                          top_prefixes_input, output_name)

                    pool.apply_async(
                        display_detectable_info,
                        (detectable_prefixes_input, top_prefixes_input,
                         output_name, top_prefixes_to_check, speed, loss_rates,
                         max_trees, max_combinations),
                        {})
                    #display_detectable_info(detectable_prefixes_input, top_prefixes_input, output_name, top_prefixes_to_check, speed, loss_rates, max_trees, max_combinations)


"""
Comparing systems at a higher level
"""


def get_many_runs_performance(
        path_to_outputs, matching_features,
        x_axis={"NumTopFails": ["1", "10", "50", "100"]}):
    """
    ASSUMPTION THIS TESTS USE THE SAME TRACE AND SLICE
    Args:
        path_to_outputs:
        matching_features:

    Returns:

    """

    # We get basic info since its common with all these traces...
    # runs
    runs_info = get_specific_tests_info(path_to_outputs, matching_features[0])

    # load first info file
    sim_info = load_sim_info_file(runs_info[0])
    inputs_dir_base = sim_info["InDirBase"]
    slice = int(sim_info["TraceSlice"])

    # load info
    slice_top_prefixes = load_top_prefixes_dict(
        inputs_dir_base + "_{}.top".format(slice))
    global_top_prefixes = load_top_prefixes_dict(inputs_dir_base + ".top")

    trace_ts = load_trace_ts(inputs_dir_base + "_{}.info".format(slice))
    prefixes_ts = load_prefixes_ts_raw(
        inputs_dir_base + "_{}.ts".format(slice))

    start_ts = trace_ts[0]
    end_ts = start_ts + Decimal(sim_info["SendDuration"])

    output_performance = []

    for matching_feature in matching_features:

        performances = {}

        for x_element in list(x_axis.values())[0]:

            _matching_feature = matching_feature.copy()
            # adds NumTopFails
            _matching_feature[list(x_axis.keys())[0]] = str(x_element)

            runs_info = get_specific_tests_info(
                path_to_outputs, _matching_feature)

            print(runs_info)

            avg_avg_detection = 0
            avg_median_detection = 0
            avg_95_detection = 0
            percentile_95_of_medians = []

            avg_tpr = 0
            avg_fpr = 0
            avg_tp = 0
            avg_fp = 0
            avg_slice_bytes = {'total': 0, "top_entry": 0, "zoomed": 0}

            for run_info in runs_info:

                print(run_info)

                sim_info = load_sim_info_file(run_info)
                outputs_dir_base = sim_info["OutDirBase"]
                num_top_prefixes = int(sim_info["NumTopEntriesSystem"])

                failed_prefixes = load_failed_prefixes(
                    outputs_dir_base + "-failed_prefixes.txt")
                sim_out = load_simulation_out(outputs_dir_base + "_s1.json")

                # Get Detection times
                #res = get_prefixes_detection_times(sim_out, failed_prefixes, prefixes_ts, global_top_prefixes, slice_top_prefixes, start_ts, num_top_prefixes)

                res = get_prefixes_detection_times(
                    sim_out, failed_prefixes, prefixes_ts, global_top_prefixes,
                    slice_top_prefixes, start_ts, end_ts, num_top_prefixes,
                    int(sim_info["TreeDepth"]),
                    Decimal(sim_info["ProbingTimeZoomingMs"]) / 1000)

                detection_times, prefix_to_info, * \
                    cdfs, avg_detection, median_detection, percentile_95 = get_detection_times_cdf_info(res)
                avg_avg_detection += avg_detection
                avg_median_detection += median_detection
                percentile_95_of_medians.append(median_detection)
                avg_95_detection += percentile_95
                # Get Performances
                tpr, fpr, tnr, fnr, tp, fp = get_run_performance(
                    sim_out, failed_prefixes, len(slice_top_prefixes),
                    global_top_prefixes, num_top_prefixes)
                avg_tpr += tpr
                avg_fpr += fpr
                avg_tp += tp
                avg_fp += fp

                # Get Shares
                detection_shares = get_detection_shares_info(
                    sim_out, failed_prefixes, slice_top_prefixes,
                    global_top_prefixes, num_top_prefixes)
                avg_slice_bytes['total'] += detection_shares['slice']['bytes']['total']
                avg_slice_bytes['top_entry'] += detection_shares['slice']['bytes']['top_entry']
                avg_slice_bytes['zoomed'] += detection_shares['slice']['bytes']['zoomed']

            N = len(runs_info)
            avg_avg_detection = avg_avg_detection / N
            avg_median_detection = avg_median_detection / N
            # all medians
            percentile_95_of_medians = np.percentile(
                percentile_95_of_medians, 95)
            avg_95_detection = avg_95_detection / N
            avg_tpr = avg_tpr / N
            avg_fpr = avg_fpr / N
            avg_tp = avg_tp / N
            avg_fp = avg_fp / N
            avg_slice_bytes['total'] = avg_slice_bytes['total'] / N
            avg_slice_bytes['top_entry'] = avg_slice_bytes['top_entry'] / N
            avg_slice_bytes['zoomed'] = avg_slice_bytes['zoomed'] / N
            # ipdb.set_trace()

            performances[x_element] = {
                'avg_avg_detection': avg_avg_detection,
                'avg_median_detection': avg_median_detection,
                'avg_tpr': avg_tpr,
                'avg_fpr': avg_fpr,
                'avg_tp': avg_tp,
                'avg_fp': avg_fp,
                'avg_slice_bytes': avg_slice_bytes,
                'avg_95_detection': avg_95_detection,
                'percentile_95_of_medians': percentile_95_of_medians,
                'num_runs': len(runs_info)
            }

        output_performance.append((matching_feature, performances))

    return output_performance


def run_plot_systems_comparison(
        experiments_path, system_spec, figure_output_path, info,
        x_axis={"NumTopFails": ["1", "10", "50", "100"]}):
    print(system_spec)
    out = get_many_runs_performance(experiments_path, system_spec, x_axis)
    plot_systems_comparison(out, figure_output_path, info)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--visualize', help="What to visualize",
                        type=str, required=False, default="")
    parser.add_argument('--info', help="Path to info",
                        type=str, required=False, default="")
    args = parser.parse_args()

    """
    Shows prefix by prefix info from a single run. At the end prints some stats.
    """
    if args.visualize == "single_run":
        check_prefix_by_prefix_from_info(args.info)

    # Compares and plots multiple runs and makes averages etc.
    elif args.visualize == "multi_runs":
        # "ProbingTimeZoomingMs": probing_time
        different_system_comparisons = {"1MB":
                                        [
                                            {
                                                "NumTopEntriesSystem": '1000',
                                                "TreeDepth": '0',
                                                "LayerSplit": '0',
                                                "CounterWidth": '0',
                                                "TreeEnabled": '0'},
                                            {
                                                "NumTopEntriesSystem": '0',
                                                "TreeDepth": '3',
                                                "LayerSplit": '3',
                                                "CounterWidth": '205',
                                                "TreeEnabled": '1'},
                                            {
                                                "NumTopEntriesSystem": '660',
                                                "TreeDepth": '3',
                                                "LayerSplit": '2',
                                                "CounterWidth": '128',
                                                "TreeEnabled": '1'}
                                        ],

                                        "500KB":
                                            [
                                                {"NumTopEntriesSystem": '500', "TreeDepth": '0', "LayerSplit": '0', "CounterWidth": '0',
                                                 "TreeEnabled": '0'},
                                                {"NumTopEntriesSystem": '0', "TreeDepth": '3', "LayerSplit": '2', "CounterWidth": '190',
                                                 "TreeEnabled": '1'},
                                                {"NumTopEntriesSystem": '250', "TreeDepth": '3', "LayerSplit": '2', "CounterWidth": '100',
                                                 "TreeEnabled": '1'}
                                        ],
                                        #
                                        "250KB":
                                            [
                                                {"NumTopEntriesSystem": '250', "TreeDepth": '0', "LayerSplit": '0', "CounterWidth": '0',
                                                 "TreeEnabled": '0'},
                                                {"NumTopEntriesSystem": '0', "TreeDepth": '4', "LayerSplit": '2', "CounterWidth": '44',
                                                 "TreeEnabled": '1'},
                                                {"NumTopEntriesSystem": '100', "TreeDepth": '4', "LayerSplit": '2', "CounterWidth": '28',
                                                 "TreeEnabled": '1'}
                                        ],
                                        "250KB(2)":
                                            [
                                                {"NumTopEntriesSystem": '250', "TreeDepth": '0', "LayerSplit": '0', "CounterWidth": '0',
                                                 "TreeEnabled": '0'},
                                                {"NumTopEntriesSystem": '0', "TreeDepth": '4', "LayerSplit": '2', "CounterWidth": '44',
                                                 "TreeEnabled": '1'},
                                                {"NumTopEntriesSystem": '180', "TreeDepth": '4', "LayerSplit": '1', "CounterWidth": '45',
                                                 "TreeEnabled": '1'}
                                        ]
                                        }
#
        probing_times = ["10.000000", "50.000000",
                         "100.000000", "150.000000", "200.000000"]
        pool = multiprocessing.Pool(20)

        for memory_usage, system_spec in different_system_comparisons.items():
            for probing_time in probing_times:

                # adds zooming time to tree based ones
                system_spec[1]["ProbingTimeZoomingMs"] = probing_time
                system_spec[2]["ProbingTimeZoomingMs"] = probing_time

                # I copy it since apply_async is taking the same variable from memory even after I modify it... INCREDIBLE
                _system_spec = copy.deepcopy(system_spec)

                out_figure_path = "/mnt/fischer/cedgar/fancy_outputs/design_experiments_1_figures/system_design_{}_{}".format(
                    memory_usage, int(float(probing_time)))
                info = {"zooming_speed": probing_time,
                        "memory_usage": memory_usage}
                _info = info.copy()

                # print(system_spec)
                #out = get_many_runs_performance("/mnt/fischer/cedgar/fancy_outputs/design_experiments_1", system_spec)
                #plot_systems_comparison(out, out_figure_path, info)

                #run_plot_systems_comparison("/mnt/fischer/cedgar/fancy_outputs/design_experiments_1", _system_spec, out_figure_path, _info)
                pool.apply_async(
                    run_plot_systems_comparison,
                    ("/mnt/fischer/cedgar/fancy_outputs/design_experiments_1",
                     _system_spec, out_figure_path, _info),
                    {})
                #import time; time.sleep(5)

    # Just loads this file so we can use it in interactive mode
    else:
        pass
